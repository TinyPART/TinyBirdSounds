{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63721a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf607072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "\n",
    "class ModelType(Enum):\n",
    "    TORCH = auto()\n",
    "    TFLITE = auto()\n",
    "\n",
    "class InputType(Enum):\n",
    "    MEL_SPEC = auto()\n",
    "    TIME_SERIES = auto()\n",
    "\n",
    "class ModelMeta:\n",
    "    def __init__(self, name, path, model_type, input_type,quantized=False):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.type = model_type\n",
    "        self.input_type = input_type\n",
    "        self.quantized = quantized\n",
    "\n",
    "#1 - Target, 0 - Non-target\n",
    "def calculate_binary_classification_performance(predicted_labels, true_labels):\n",
    "    y_true, y_pred = true_labels, predicted_labels\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    f2 = (1 + 2**2) * (precision * recall) / (2**2 * precision + recall)\n",
    "\n",
    "    return {\n",
    "        \"TP\": tp,\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"TPR\": recall,\n",
    "        \"FPR\": fp / (fp + tn),\n",
    "        \"F1\": f1,\n",
    "        \"F2\": f2\n",
    "    }\n",
    "\n",
    "# data -> model -> softmax\n",
    "# return softmax preds val idx 1 - Target, 0 - Non-target\n",
    "#        true_labels\n",
    "def inference_torch_model(model, data_loader, quantized=False):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    prediction_softmax = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            prediction_softmax.append(preds)\n",
    "            true_labels.append(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "    prediction_softmax = np.concatenate(prediction_softmax)\n",
    "    true_labels = np.concatenate(true_labels)\n",
    "    prediction_softmax[:, [0, 1]] = prediction_softmax[:, [1, 0]]\n",
    "    true_labels = 1 - true_labels\n",
    "    return prediction_softmax, true_labels\n",
    "\n",
    "def evaluate_torch_model(model, data_loader, quantized=False, threshold_vals=None):\n",
    "    prediction_score, true_labels = inference_torch_model(model, data_loader, quantized)\n",
    "    metrics_on_t = []\n",
    "    for t in threshold_vals:\n",
    "        preds = prediction_score[:, 1]\n",
    "        preds = preds > t\n",
    "        metrics = calculate_binary_classification_performance(preds, true_labels)\n",
    "        metrics['t'] = t\n",
    "        metrics_on_t.append(metrics)\n",
    "    return metrics_on_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = [\n",
    "    ModelMeta(\"StreamingCNNTiny\", \n",
    "              \"./AudioClassifierCNNNoReluTiny_16K_3s_raw_signal/model.51.0.9442029595375061.pth\",\n",
    "              ModelType.TORCH, InputType.TIME_SERIES\n",
    "             ),\n",
    "    ModelMeta(\"StreamingTransformer\", \"./model_transformer.pth\", ModelType.TORCH, InputType.TIME_SERIES),\n",
    "    \n",
    "    ModelMeta(\"SqueezNet-Time-Series\", \n",
    "              \"./tf_implementation/time_series_models_to_test_with_RIOT_ML/squeezenet/squeezenet30%_time_series_16kHz_full_int_q.tflite\", \n",
    "              ModelType.TFLITE, InputType.TIME_SERIES, quantized=True\n",
    "             ),\n",
    "    \n",
    "    ModelMeta(\"CNN-Mel-Spec\", \"./tf_implementation/spectrogram_models_to_test_with_RIOT_ML/cnn/cnn_mel_spec_16kHz_full_int_q.tflite\", \n",
    "              ModelType.TFLITE, InputType.MEL_SPEC, quantized=True\n",
    "             ),\n",
    "    \n",
    "    ModelMeta(\"SqueezNet-Mel-Spec\", \n",
    "              \"./tf_implementation/spectrogram_models_to_test_with_RIOT_ML/squeezenet/squeezenet_spec_16kHz_full_int_q.tflite\", \n",
    "              ModelType.TFLITE, InputType.MEL_SPEC, quantized=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch init of data and model class\n",
    "import librosa\n",
    "def resample_audio(y, orig_sr, target_sr):\n",
    "    y = librosa.resample(y, orig_sr=orig_sr, target_sr=target_sr, res_type='zero_order_hold')\n",
    "    return y\n",
    "\n",
    "from CNNModels import AudioClassifierCNNNoRelu, AudioClassifierCNNNoReluSmall, AudioClassifierCNNNoReluTiny, AudioClassifierCNNNoReluPico\n",
    "from TransformerModel import RawAudioTransformerModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from AudioDataset import AudioDataset, RawAudioDataset\n",
    "\n",
    "train_target_folder = \"../BirdSound/roman_data/cut-data/training/target/\"\n",
    "train_non_target_folder = \"../BirdSound/roman_data/cut-data/training/non_target/\"\n",
    "\n",
    "validation_target_folder = \"../BirdSound/roman_data/cut-data/validation/target/\"\n",
    "validation_non_target_folder = \"../BirdSound/roman_data/cut-data/validation/non_target/\"\n",
    "\n",
    "test_target_folder = \"../BirdSound/roman_data/cut-data/testing/target/\"\n",
    "test_non_target_folder = \"../BirdSound/roman_data/cut-data/testing/non_target/\"\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "RESAMPLE_RATE = 16000\n",
    "FIXED_LEN_IN_SEC = 3\n",
    "FIXED_LEN_WAVE = RESAMPLE_RATE * FIXED_LEN_IN_SEC\n",
    "NUM_WORKERS = 10\n",
    "BATCH_SIZE = 256\n",
    "RESAMPLE_TRANSFORM = lambda x : torch.from_numpy(resample_audio(x.numpy(), SAMPLE_RATE, RESAMPLE_RATE))\n",
    "\n",
    "\n",
    "train_dataset = RawAudioDataset(train_target_folder, train_non_target_folder,fixed_length_wave= FIXED_LEN_WAVE, transform=RESAMPLE_TRANSFORM)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers=NUM_WORKERS)\n",
    "\n",
    "validation_dataset = RawAudioDataset(validation_target_folder, validation_non_target_folder,fixed_length_wave = FIXED_LEN_WAVE, transform=RESAMPLE_TRANSFORM)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE,  num_workers = NUM_WORKERS, shuffle = True)\n",
    "\n",
    "test_dataset = RawAudioDataset(test_target_folder, test_non_target_folder,fixed_length_wave = FIXED_LEN_WAVE, transform=RESAMPLE_TRANSFORM)\n",
    "test_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE,  num_workers = NUM_WORKERS, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from tf_implementation.helper_functions import (\n",
    "    create_spectrogram_features,\n",
    "    lite_model_from_file_predicts_dataset,\n",
    ")\n",
    "\n",
    "# Take all audio from testing dataset and create spectrograms from them\n",
    "# We will use spectrograms for models testing\n",
    "directory = '../BirdSound/roman_data/cut-data/testing/'\n",
    "\n",
    "#create mel-spec\n",
    "\n",
    "# x_data = []\n",
    "# y_data = []\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         full_file_name = os.path.join(root, file)\n",
    "#         if \"non_target\" in str(full_file_name):\n",
    "#             class_encoded = 0\n",
    "#         elif \"target\" in str(full_file_name):\n",
    "#             class_encoded = 1\n",
    "\n",
    "#         audio, sr = tf.audio.decode_wav(tf.io.read_file(full_file_name))\n",
    "#         audio = tf.squeeze(audio, axis=-1)\n",
    "#         resampled_audio = tfio.audio.resample(audio, rate_in=SAMPLE_RATE, rate_out=RESAMPLE_RATE)\n",
    "#         # Prepare log mel spectrogram from audio\n",
    "#         spectrogram_feature = create_spectrogram_features(resampled_audio, desired_length=FIXED_LEN_WAVE, sample_rate = RESAMPLE_RATE)\n",
    "#         x_data.append(spectrogram_feature)\n",
    "#         y_data.append(class_encoded)\n",
    "\n",
    "# # input data should be in numpy array, not in list\n",
    "# x_data_mel = np.array(x_data)\n",
    "# y_data_labels = np.array(y_data)\n",
    "\n",
    "# np.save(\"x_data_mel_test_tf.npy\", x_data_mel)\n",
    "# np.save(\"y_data_labels_test_tf.npy\", y_data_labels)\n",
    "\n",
    "\n",
    "#create raw audio\n",
    "# x_data = []\n",
    "# y_data = []\n",
    "# desired_length_of_audio = FIXED_LEN_WAVE\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         full_file_name = os.path.join(root, file)\n",
    "\n",
    "#         if \"non_target\" in str(full_file_name):\n",
    "#             class_encoded = 0\n",
    "#         elif \"target\" in str(full_file_name):\n",
    "#             class_encoded = 1\n",
    "\n",
    "#         audio, sr = tf.audio.decode_wav(tf.io.read_file(full_file_name))\n",
    "#         audio = tf.squeeze(audio, axis=-1)\n",
    "#         resampled_audio = tfio.audio.resample(audio, rate_in=SAMPLE_RATE, rate_out=RESAMPLE_RATE)\n",
    "#         audio_length = tf.shape(resampled_audio)[0]\n",
    "#         if audio_length < desired_length_of_audio:\n",
    "#             resampled_audio = tf.pad(resampled_audio, [[0, desired_length_of_audio - audio_length]], mode='CONSTANT')\n",
    "#         else:\n",
    "#             resampled_audio = resampled_audio[:desired_length_of_audio]\n",
    "#         resampled_audio = tf.expand_dims(resampled_audio, axis=-1).numpy()\n",
    "\n",
    "#         x_data.append(resampled_audio)\n",
    "#         y_data.append(class_encoded)\n",
    "\n",
    "# # input data should be in numpy array, not in list\n",
    "# x_data_raw = np.array(x_data)\n",
    "# y_data_raw = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e306d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_mel_train = np.load(\"x_data_mel_train_tf.npy\")\n",
    "x_data_ts_train = np.load(\"x_data_ts_train_tf.npy\")\n",
    "y_data_labels_train = np.load(\"y_data_labels_train_tf.npy\")\n",
    "x_data_mel_test = np.load(\"x_data_mel_test_tf.npy\")\n",
    "x_data_ts_test = np.load(\"x_data_ts_test_tf.npy\")\n",
    "y_data_labels_test = np.load(\"y_data_labels_test_tf.npy\")\n",
    "\n",
    "def run_full_int_q_tflite_model(tflite_file, test_image_indices, x_data):\n",
    "    # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    predictions = np.zeros((len(test_image_indices),), dtype=float)\n",
    "    for i, test_image_index in enumerate(test_image_indices):\n",
    "        test_data_point = x_data[test_image_index]\n",
    "\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_data_point = test_data_point / input_scale + input_zero_point\n",
    "\n",
    "        test_data_point = np.expand_dims(test_data_point, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_data_point)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "        predictions[i] = tf.nn.softmax(tf.cast(output, tf.float32))[1]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def full_int_model_predict(tflite_file, x_data):\n",
    "    test_image_indices = range(len(x_data))\n",
    "    predictions = run_full_int_q_tflite_model(tflite_file, test_image_indices, x_data)\n",
    "    return predictions\n",
    "\n",
    "# data -> model -> softmax\n",
    "# return softmax validx 1 - Target, 0 - Non-target\n",
    "def inference_tflite_model(model_path, data, quantized=False):\n",
    "    if quantized:\n",
    "        prediction_score = full_int_model_predict(model_path, data)\n",
    "    return prediction_score\n",
    "\n",
    "\n",
    "def evaluate_tflite_model(model_path, x_data, y_data, quantized=False, threshold_vals=None):\n",
    "    prediction_score, true_labels = inference_tflite_model(model_path, x_data, quantized), y_data\n",
    "    metrics_on_t = []\n",
    "    for t in threshold_vals:\n",
    "        preds = prediction_score\n",
    "        preds = preds >= t\n",
    "        metrics = calculate_binary_classification_performance(preds, true_labels)\n",
    "        metrics['t'] = t\n",
    "        metrics_on_t.append(metrics)\n",
    "    return metrics_on_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126aab2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Metrics on training dataset\n",
    "\n",
    "\n",
    "t = [x * 0.01 for x in range(0, 101)]\n",
    "\n",
    "metrics_diff_models = []\n",
    "\n",
    "for meta in metas:\n",
    "    if meta.type == ModelType.TORCH:\n",
    "        if meta.name == \"StreamingCNNTiny\":\n",
    "            model = AudioClassifierCNNNoReluTiny()\n",
    "        if meta.name == \"StreamingTransformer\":\n",
    "            model = RawAudioTransformerModel(num_classes=2, n_embd=16, n_head=1, block_size=16, hidden_size=32, n_layers=1)\n",
    "        model.load_state_dict(torch.load(meta.path))\n",
    "        metrics = evaluate_torch_model(model, train_dataloader, False, t)\n",
    "\n",
    "    elif meta.type == ModelType.TFLITE:\n",
    "        if meta.input_type == InputType.MEL_SPEC:\n",
    "            metrics = evaluate_tflite_model(meta.path, x_data_mel_train, y_data_labels_train, meta.quantized, t)\n",
    "        elif meta.input_type == InputType.TIME_SERIES:\n",
    "            metrics = evaluate_tflite_model(meta.path, x_data_ts_train, y_data_labels_train, meta.quantized, t)\n",
    "    metrics_diff_models.append({'name': meta.name, 'path':  meta.path, 'metrics': metrics})\n",
    "import pickle\n",
    "with open('metrics_diff_models_train.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics_diff_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics on test dataset\n",
    "\n",
    "\n",
    "t = [x * 0.01 for x in range(0, 101)]\n",
    "\n",
    "metrics_diff_models = []\n",
    "\n",
    "for meta in metas:\n",
    "    if meta.type == ModelType.TORCH:\n",
    "        if meta.name == \"StreamingCNNTiny\":\n",
    "            model = AudioClassifierCNNNoReluTiny()\n",
    "        if meta.name == \"StreamingTransformer\":\n",
    "            model = RawAudioTransformerModel(num_classes=2, n_embd=16, n_head=1, block_size=16, hidden_size=32, n_layers=1)\n",
    "        model.load_state_dict(torch.load(meta.path))\n",
    "        metrics = evaluate_torch_model(model, test_dataloader, False, t)\n",
    "\n",
    "    elif meta.type == ModelType.TFLITE:\n",
    "        if meta.input_type == InputType.MEL_SPEC:\n",
    "            metrics = evaluate_tflite_model(meta.path, x_data_mel_test, y_data_labels_test, meta.quantized, t)\n",
    "        elif meta.input_type == InputType.TIME_SERIES:\n",
    "            metrics = evaluate_tflite_model(meta.path, x_data_ts_test, y_data_labels_test, meta.quantized, t)\n",
    "    metrics_diff_models.append({'name': meta.name, 'path':  meta.path, 'metrics': metrics})\n",
    "\n",
    "# np.save(\"metrics_diff_models_test.npy\", metrics_diff_models)\n",
    "import pickle\n",
    "with open('metrics_diff_models_test.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics_diff_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8daccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transformer = RawAudioTransformerModel(num_classes=2, n_embd=16, n_head=1, block_size=16, hidden_size=32, n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(audio_transformer, input_size=(1, 1, 48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb170056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
